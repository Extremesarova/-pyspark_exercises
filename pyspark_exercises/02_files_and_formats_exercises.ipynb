{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1cb532b",
   "metadata": {
    "id": "b1cb532b",
    "papermill": {
     "duration": 0.023342,
     "end_time": "2021-07-13T06:13:53.247916",
     "exception": false,
     "start_time": "2021-07-13T06:13:53.224574",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<center>\n",
    "    This is Set 2: Files and Formats (Exercises 11-20)\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dfad97",
   "metadata": {
    "id": "44dfad97",
    "papermill": {
     "duration": 0.023132,
     "end_time": "2021-07-13T06:13:53.295747",
     "exception": false,
     "start_time": "2021-07-13T06:13:53.272615",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Prerequisites**\n",
    "* Additional package `pyarrow` should be installed.\n",
    "* Small example values along with the sample dataset [datatableton_sample.zip](https://github.com/vopani/datatableton/blob/main/data/datatableton_sample.zip) will be used for the exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad8ddf33",
   "metadata": {
    "_kg_hide-output": true,
    "id": "ad8ddf33",
    "outputId": "18f6a5eb-11b7-4f1c-d17c-a4f1b9fac5ec",
    "papermill": {
     "duration": 57.719478,
     "end_time": "2021-07-13T06:14:51.038854",
     "exception": false,
     "start_time": "2021-07-13T06:13:53.319376",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip3 install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "824b1fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb95133b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/20 10:26:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ab51cc31",
   "metadata": {
    "id": "ab51cc31",
    "outputId": "d840bb66-de13-4901-eb31-6669fe0ab41d",
    "papermill": {
     "duration": 0.30217,
     "end_time": "2021-07-13T06:14:51.375337",
     "exception": false,
     "start_time": "2021-07-13T06:14:51.073167",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.parallelize(list(zip(list(range(0, 11)), list(\"NEVERGIVEUP\"))))\n",
    "dframe = spark.createDataFrame(data=rdd, schema=[\"v1\", \"v2\"])\n",
    "\n",
    "dframe_pd = pd.DataFrame({'v1': range(11), 'v2': list(\"NEVERGIVEUP\")})\n",
    "nparray = np.array([[0, 'C'], [1, 'O'], [2, 'D'], [3, 'E']])\n",
    "patable = pa.Table.from_pandas(dframe_pd)\n",
    "dlist = [list(range(4)), ['D', 'A', 'T', 'A']]\n",
    "ddict = {'x': list(range(6)), 'y': ['P', 'Y', 'T', 'H', 'O', 'N']}\n",
    "dtup = [(0, 'L'), (1, 'E'), (2, 'A'), (3, 'R'), (4, 'N')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99527782",
   "metadata": {
    "id": "99527782",
    "papermill": {
     "duration": 0.033194,
     "end_time": "2021-07-13T06:14:51.442082",
     "exception": false,
     "start_time": "2021-07-13T06:14:51.408888",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Exercise 11: Write `dframe` to a csv named `data1.csv`, to a compressed gz csv named `data.gz`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79df5d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| v1| v2|\n",
      "+---+---+\n",
      "|  0|  N|\n",
      "|  1|  E|\n",
      "|  2|  V|\n",
      "|  3|  E|\n",
      "|  4|  R|\n",
      "|  5|  G|\n",
      "|  6|  I|\n",
      "|  7|  V|\n",
      "|  8|  E|\n",
      "|  9|  U|\n",
      "| 10|  P|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d47f3504",
   "metadata": {},
   "outputs": [],
   "source": [
    "dframe_schema = dframe.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35292732",
   "metadata": {
    "id": "35292732",
    "papermill": {
     "duration": 0.033376,
     "end_time": "2021-07-13T06:14:51.510057",
     "exception": false,
     "start_time": "2021-07-13T06:14:51.476681",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dframe.write.csv(\"data1\", header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8e77b7f-b7e1-4193-8109-b0b2e3ea4128",
   "metadata": {},
   "outputs": [],
   "source": [
    "dframe.write.csv(\"data2\", header=True, mode=\"overwrite\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c950057",
   "metadata": {
    "id": "9c950057",
    "papermill": {
     "duration": 0.032872,
     "end_time": "2021-07-13T06:14:51.577603",
     "exception": false,
     "start_time": "2021-07-13T06:14:51.544731",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Exercise 12: Read `data1.csv` and assign it to `data_csv`, read `data.gz` and assign it to `data_gz`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "501fa96e",
   "metadata": {
    "id": "501fa96e",
    "papermill": {
     "duration": 0.034001,
     "end_time": "2021-07-13T06:14:51.646083",
     "exception": false,
     "start_time": "2021-07-13T06:14:51.612082",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| v1| v2|\n",
      "+---+---+\n",
      "|  9|  U|\n",
      "| 10|  P|\n",
      "|  2|  V|\n",
      "|  3|  E|\n",
      "|  6|  I|\n",
      "|  7|  V|\n",
      "|  0|  N|\n",
      "|  1|  E|\n",
      "|  4|  R|\n",
      "|  5|  G|\n",
      "|  8|  E|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_csv = spark.read.options(header=True).schema(dframe_schema).csv(\"data1\")\n",
    "data_csv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ba85b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- v1: long (nullable = true)\n",
      " |-- v2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_csv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d5a8154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| v1| v2|\n",
      "+---+---+\n",
      "|  0|  N|\n",
      "|  1|  E|\n",
      "|  2|  V|\n",
      "|  3|  E|\n",
      "|  4|  R|\n",
      "|  5|  G|\n",
      "|  6|  I|\n",
      "|  7|  V|\n",
      "|  8|  E|\n",
      "|  9|  U|\n",
      "| 10|  P|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_csv.sort(\"v1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f178bd70",
   "metadata": {
    "id": "f178bd70",
    "papermill": {
     "duration": 0.034641,
     "end_time": "2021-07-13T06:14:51.714948",
     "exception": false,
     "start_time": "2021-07-13T06:14:51.680307",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| v1| v2|\n",
      "+---+---+\n",
      "|  9|  U|\n",
      "| 10|  P|\n",
      "|  2|  V|\n",
      "|  3|  E|\n",
      "|  6|  I|\n",
      "|  7|  V|\n",
      "|  8|  E|\n",
      "|  5|  G|\n",
      "|  0|  N|\n",
      "|  4|  R|\n",
      "|  1|  E|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_gz = spark.read.options(header=True).schema(dframe_schema).csv(\"data2\")\n",
    "data_gz.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4d4fba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- v1: long (nullable = true)\n",
      " |-- v2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_gz.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a046b10d",
   "metadata": {
    "id": "a046b10d",
    "papermill": {
     "duration": 0.033938,
     "end_time": "2021-07-13T06:14:51.851512",
     "exception": false,
     "start_time": "2021-07-13T06:14:51.817574",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Exercise 13: Read data from this URL: [https://raw.githubusercontent.com/vopani/datatableton/main/data/datatableton_sample.csv](https://raw.githubusercontent.com/vopani/datatableton/main/data/datatableton_sample.csv) and assign it to `data_url`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea7d3ba6",
   "metadata": {
    "id": "ea7d3ba6",
    "papermill": {
     "duration": 0.03297,
     "end_time": "2021-07-13T06:14:51.917837",
     "exception": false,
     "start_time": "2021-07-13T06:14:51.884867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+----+-------+--------+-----+--------+\n",
      "|          timestamp|user| age|product|category|price|quantity|\n",
      "+-------------------+----+----+-------+--------+-----+--------+\n",
      "|2017-01-01 13:22:41|  U1|  21|   Eggs|   Dairy|  2.3|       2|\n",
      "|2017-05-22 09:54:21|  U2|  41|  Bread|      NA|  0.6|       2|\n",
      "|2018-11-09 15:00:01|  U3|null| Banana|   Fruit|  0.7|       1|\n",
      "|2018-12-24 23:03:33|  U1|  21|  Water|   Drink|  0.2|       1|\n",
      "|2019-03-03 06:21:58|  U4|  19|   Eggs|   Dairy|  2.3|       2|\n",
      "|2019-06-17 16:13:39|  U5|  25| Grapes|   Fruit|  1.5|       1|\n",
      "|2019-07-28 21:03:11|  U5|  25|  Bread|      NA|  0.6|       1|\n",
      "|2019-12-05 04:15:42|  U1|  21| Banana|   Fruit|  0.7|       2|\n",
      "|2020-02-02 03:45:34|  U3|null| Banana|   Fruit|  0.7|       1|\n",
      "|2020-03-05 07:09:12|  U4|  19| Grapes|   Fruit|  1.5|       1|\n",
      "|2020-03-22 19:29:38|  U1|  21|  Water|   Drink|  0.2|       1|\n",
      "|2020-03-30 09:44:30|  U4|  19|  Water|   Drink|  0.2|       1|\n",
      "|2020-04-01 13:21:41|  U1|  21| Banana|   Fruit|  0.7|       2|\n",
      "|2020-07-08 11:45:25|  U2|  41| Grapes|   Fruit|  1.5|       1|\n",
      "|2020-11-19 18:51:22|  U5|  25|  Water|   Drink|  0.2|       1|\n",
      "|2020-12-03 16:23:48|  U3|null| Banana|   Fruit|  0.5|       3|\n",
      "|2021-02-03 01:14:40|  U5|  25|   Eggs|   Dairy|  2.1|       4|\n",
      "|2021-05-26 22:42:15|  U3|null|  Bread|      NA|  0.6|       1|\n",
      "|2021-06-14 15:49:28|  U4|  19|   Eggs|   Dairy|  2.3|       2|\n",
      "|2021-07-01 04:37:31|  U4|  19|  Water|   Drink|  0.3|       1|\n",
      "+-------------------+----+----+-------+--------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/vopani/datatableton/main/data/datatableton_sample.csv\"\n",
    "from pyspark import SparkFiles\n",
    "spark.sparkContext.addFile(url)\n",
    " \n",
    "path  = SparkFiles.get('datatableton_sample.csv')\n",
    "data_url = spark.read.csv(\"file://\" + path, header=True, inferSchema= True, sep = \",\")\n",
    "data_url.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2f2948",
   "metadata": {
    "id": "eb2f2948",
    "papermill": {
     "duration": 0.034549,
     "end_time": "2021-07-13T06:14:51.987099",
     "exception": false,
     "start_time": "2021-07-13T06:14:51.952550",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Exercise 14: Read `users.csv` from `datatableton_sample.zip` and assign it to `data_zip`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bd29f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = ZipFile('datatableton_sample.zip')\n",
    "csv = [l.decode('utf-8').replace('\\n', '') for l in z.open('users.csv').readlines()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f578887f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_zip_rdd = (spark\n",
    "    .sparkContext\n",
    "    .parallelize(csv)\n",
    ")\n",
    "rows = data_zip_rdd.map(lambda x: x.replace(\"\\r\", \"\").split(\",\")).collect()\n",
    "headers = rows[0]\n",
    "rdd = spark.sparkContext.parallelize(rows[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3f45d839",
   "metadata": {
    "id": "3f45d839",
    "papermill": {
     "duration": 0.033375,
     "end_time": "2021-07-13T06:14:52.054323",
     "exception": false,
     "start_time": "2021-07-13T06:14:52.020948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+---------+-------+\n",
      "| ﻿Person|Category ID|Return ID| Region|\n",
      "+--------+-----------+---------+-------+\n",
      "|     Amy|       1001|     R004|   West|\n",
      "|     Max|       1002|     R001|   East|\n",
      "| William|       1007|     R003|Central|\n",
      "|   Nancy|       1004|     R006|  South|\n",
      "|Isabella|       1001|     R006|   West|\n",
      "|     Tom|       1001|     R005|   East|\n",
      "|    Mary|       1007|     R005|  South|\n",
      "|  Carlos|       1008|     R006|   West|\n",
      "|    Kimi|       1003|     R002|   East|\n",
      "|   Lando|       1002|     R003|Central|\n",
      "|   Perez|       1001|     R001|   East|\n",
      "|   Lewis|       1003|     R001|  South|\n",
      "|  Monica|       1005|     R002|   West|\n",
      "|   Carla|       1001|     R007|   East|\n",
      "+--------+-----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_zip = spark.createDataFrame(data=rdd, schema=headers)\n",
    "data_zip.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ede47b-7e6f-4afa-96b6-b64a15d9b1d8",
   "metadata": {},
   "source": [
    "**Exercise 15: Create a PySpark DataFrame `data_pd` from the pd.DataFrame `dframe` and create a pd.DataFrame `pd_data` from the PySpark DataFrame `data_pd`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7a8952b2-269e-45c1-bb80-294391753036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| v1| v2|\n",
      "+---+---+\n",
      "|  0|  N|\n",
      "|  1|  E|\n",
      "|  2|  V|\n",
      "|  3|  E|\n",
      "|  4|  R|\n",
      "|  5|  G|\n",
      "|  6|  I|\n",
      "|  7|  V|\n",
      "|  8|  E|\n",
      "|  9|  U|\n",
      "| 10|  P|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_pd = spark.createDataFrame(dframe_pd)\n",
    "data_pd.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9d606447-9133-402c-a2e7-a7ad59a92775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    v1 v2\n",
       "0    0  N\n",
       "1    1  E\n",
       "2    2  V\n",
       "3    3  E\n",
       "4    4  R\n",
       "5    5  G\n",
       "6    6  I\n",
       "7    7  V\n",
       "8    8  E\n",
       "9    9  U\n",
       "10  10  P"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_data = data_pd.toPandas()\n",
    "pd_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953480b7",
   "metadata": {
    "id": "953480b7",
    "papermill": {
     "duration": 0.033579,
     "end_time": "2021-07-13T06:14:52.335992",
     "exception": false,
     "start_time": "2021-07-13T06:14:52.302413",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Exercise 16: Create a PySpark DataFrame `data_np` from the np.array `nparray` and create a np.array `np_data` from the PySpark DataFrame `data_np`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4c9a6421-beb3-406d-947d-78d50a0ad3c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['0', 'C'],\n",
       "       ['1', 'O'],\n",
       "       ['2', 'D'],\n",
       "       ['3', 'E']], dtype='<U21')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nparray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f24a0b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| _1| _2|\n",
      "+---+---+\n",
      "|  0|  C|\n",
      "|  1|  O|\n",
      "|  2|  D|\n",
      "|  3|  E|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd = spark.sparkContext.parallelize(nparray.tolist())\n",
    "data_np = rdd.toDF()\n",
    "data_np.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "194b017c",
   "metadata": {
    "id": "194b017c",
    "papermill": {
     "duration": 0.033428,
     "end_time": "2021-07-13T06:14:52.470210",
     "exception": false,
     "start_time": "2021-07-13T06:14:52.436782",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['0', 'C'],\n",
       "       ['1', 'O'],\n",
       "       ['2', 'D'],\n",
       "       ['3', 'E']], dtype='<U1')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_data = np.array(data_np.collect())\n",
    "np_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f379c2b6",
   "metadata": {
    "id": "f379c2b6",
    "papermill": {
     "duration": 0.033474,
     "end_time": "2021-07-13T06:14:52.537451",
     "exception": false,
     "start_time": "2021-07-13T06:14:52.503977",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Exercise 17: Create a PySpark DataFrame `data_ls` from the list `dlist` and create a list `ls_data` from the PySpark DataFrame `data_ls`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a82c49b4-1f24-4d2a-989d-02d346d0bf7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3], ['D', 'A', 'T', 'A']]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "31c86602",
   "metadata": {
    "id": "31c86602",
    "papermill": {
     "duration": 0.03292,
     "end_time": "2021-07-13T06:14:52.603880",
     "exception": false,
     "start_time": "2021-07-13T06:14:52.570960",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| _1| _2|\n",
      "+---+---+\n",
      "|  0|  D|\n",
      "|  1|  A|\n",
      "|  2|  T|\n",
      "|  3|  A|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd = spark.sparkContext.parallelize(list(zip(dlist[0], dlist[1])))\n",
    "data_ls = rdd.toDF()\n",
    "data_ls.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c6938c5e",
   "metadata": {
    "id": "c6938c5e",
    "papermill": {
     "duration": 0.033088,
     "end_time": "2021-07-13T06:14:52.670815",
     "exception": false,
     "start_time": "2021-07-13T06:14:52.637727",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3], ['D', 'A', 'T', 'A']]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls_data = [data_ls.select(col).rdd.flatMap(lambda x: x).collect() for col in data_ls.columns]\n",
    "ls_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239adf2b",
   "metadata": {
    "id": "239adf2b",
    "papermill": {
     "duration": 0.033084,
     "end_time": "2021-07-13T06:14:52.737400",
     "exception": false,
     "start_time": "2021-07-13T06:14:52.704316",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Exercise 18: Create a PySpark DataFrame `data_dc` from the dictionary `ddict` and create a dictionary `dc_data` from the PySpark DataFrame `data_dc`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9e516e17-eb65-41f4-8552-7ed3d2aaa5d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': [0, 1, 2, 3, 4, 5], 'y': ['P', 'Y', 'T', 'H', 'O', 'N']}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "ec621496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  x|  y|\n",
      "+---+---+\n",
      "|  0|  P|\n",
      "|  1|  Y|\n",
      "|  2|  T|\n",
      "|  3|  H|\n",
      "|  4|  O|\n",
      "|  5|  N|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dict_values = list(ddict.values())\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(list(zip(dict_values[0], dict_values[1])))\n",
    "data_dc = rdd.toDF(list(ddict.keys()))\n",
    "data_dc.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "3bfe534a",
   "metadata": {
    "id": "3bfe534a",
    "papermill": {
     "duration": 0.03345,
     "end_time": "2021-07-13T06:14:52.873395",
     "exception": false,
     "start_time": "2021-07-13T06:14:52.839945",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': [[0, 1, 2, 3, 4, 5]], 'y': [['P', 'Y', 'T', 'H', 'O', 'N']]}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc_data = {column:data_dc.select(column).rdd.flatMap(lambda x: x).collect() for column in data_dc.columns}\n",
    "dc_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38984943",
   "metadata": {
    "id": "38984943",
    "papermill": {
     "duration": 0.033859,
     "end_time": "2021-07-13T06:14:52.943298",
     "exception": false,
     "start_time": "2021-07-13T06:14:52.909439",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Exercise 19: Create a PySpark DataFrame `data_tp` from the tuples `dtup` and create tuples `tp_data` from the PySpark DataFrame `data_tp`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ca35b095-3ef7-4e9f-ad2c-c74ac88846b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'L'), (1, 'E'), (2, 'A'), (3, 'R'), (4, 'N')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6136a2ff",
   "metadata": {
    "id": "6136a2ff",
    "papermill": {
     "duration": 0.033457,
     "end_time": "2021-07-13T06:14:53.012463",
     "exception": false,
     "start_time": "2021-07-13T06:14:52.979006",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1\n",
       "0  0  L\n",
       "1  1  E\n",
       "2  2  A\n",
       "3  3  R\n",
       "4  4  N"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tp = pd.DataFrame(dtup)\n",
    "data_tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e755f863",
   "metadata": {
    "id": "e755f863",
    "papermill": {
     "duration": 0.033086,
     "end_time": "2021-07-13T06:14:53.079613",
     "exception": false,
     "start_time": "2021-07-13T06:14:53.046527",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'L'), (1, 'E'), (2, 'A'), (3, 'R'), (4, 'N')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp_data = list(data_tp.to_records(index=False))\n",
    "tp_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1cf088",
   "metadata": {
    "id": "0d1cf088",
    "papermill": {
     "duration": 0.033454,
     "end_time": "2021-07-13T06:14:53.147465",
     "exception": false,
     "start_time": "2021-07-13T06:14:53.114011",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Exercise 20: Create a PySpark DataFrame `data_pa` from the pyarrow.Table `patable` and create a pyarrow.Table `pa_data` from the PySpark DataFrame `data_pa`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "21b160d5-3a62-4da3-8509-6ee67019f192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "v1: int64\n",
       "v2: string\n",
       "----\n",
       "v1: [[0,1,2,3,4,5,6,7,8,9,10]]\n",
       "v2: [[\"N\",\"E\",\"V\",\"E\",\"R\",\"G\",\"I\",\"V\",\"E\",\"U\",\"P\"]]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "354490db",
   "metadata": {
    "id": "354490db",
    "papermill": {
     "duration": 0.033331,
     "end_time": "2021-07-13T06:14:53.214968",
     "exception": false,
     "start_time": "2021-07-13T06:14:53.181637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    v1 v2\n",
       "0    0  N\n",
       "1    1  E\n",
       "2    2  V\n",
       "3    3  E\n",
       "4    4  R\n",
       "5    5  G\n",
       "6    6  I\n",
       "7    7  V\n",
       "8    8  E\n",
       "9    9  U\n",
       "10  10  P"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pa = patable.to_pandas()\n",
    "data_pa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e45ac22d",
   "metadata": {
    "id": "e45ac22d",
    "papermill": {
     "duration": 0.032941,
     "end_time": "2021-07-13T06:14:53.281674",
     "exception": false,
     "start_time": "2021-07-13T06:14:53.248733",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "v1: int64\n",
       "v2: string\n",
       "----\n",
       "v1: [[0,1,2,3,4,5,6,7,8,9,10]]\n",
       "v2: [[\"N\",\"E\",\"V\",\"E\",\"R\",\"G\",\"I\",\"V\",\"E\",\"U\",\"P\"]]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pa_data = pa.Table.from_pandas(data_pa)\n",
    "pa_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dccf7c",
   "metadata": {
    "id": "50dccf7c",
    "papermill": {
     "duration": 0.032929,
     "end_time": "2021-07-13T06:14:53.349446",
     "exception": false,
     "start_time": "2021-07-13T06:14:53.316517",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "✅ This completes Set 2: Files and Formats (Exercises 11-20)\n",
    "\n",
    "Original exercises for Datatable package can be found [here](https://github.com/vopani/datatableton)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "02_files_and_formats_exercises.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pyspark_exercises",
   "language": "python",
   "name": "pyspark_exercises"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 69.642344,
   "end_time": "2021-07-13T06:14:54.093505",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-07-13T06:13:44.451161",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
